{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadcsv(filename):\n",
    "    lines=csv.reader(open(filename, \"r\"))\n",
    "    dataset=list(lines)\n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i]=[float (x) for x in dataset[i]]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(dataset, splitRatio):\n",
    "    trainSize=int(len(dataset) * splitRatio)\n",
    "    trainSet=[]\n",
    "    copy=list(dataset)\n",
    "    while len(trainSet)<trainSize:\n",
    "        index=random.randrange(len(copy))\n",
    "        trainSet.append(copy.pop(index))\n",
    "    return [trainSet, copy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateByClass(dataset):\n",
    "    separated= {}\n",
    "    for i in range(len(dataset)):\n",
    "        vector=dataset[i]\n",
    "        if(vector[-1] not in separated):\n",
    "            separated[vector[-1]]=[]\n",
    "        separated[vector[-1]].append(vector)\n",
    "    return separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    "def stdev(numbers):\n",
    "    avg=mean(numbers)\n",
    "    variance=sum([pow(x-avg,2)for x in numbers])/float(len(numbers)-1)\n",
    "    return math.sqrt(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(dataset):\n",
    "    summaries=[(mean(attribute), stdev(attribute)) for attribute in zip(*dataset)]\n",
    "    del summaries[-1]\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeByClass(dataset):\n",
    "    separated=separateByClass(dataset)\n",
    "    summaries={}\n",
    "    for classValue, instances in separated.items():\n",
    "        summaries[classValue] = summarize(instances)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateProbability(x, mean, stdev):\n",
    "    exponent=math.exp(-(math.pow(x-mean,2)/(2*math.pow(stdev,2))))\n",
    "    return(1 / (math.sqrt(2*math.pi) * stdev)) * exponent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateClassProbabilities(summaries, inputVector):\n",
    "    probabilities={}\n",
    "    for classValue, classSummaries in summaries.items():\n",
    "        probabilities[classValue]=1\n",
    "        for i in range(len(classSummaries)):\n",
    "            mean, stdev=classSummaries[i]\n",
    "            x=inputVector[i]\n",
    "            probabilities[classValue] *= calculateProbability(x, mean, stdev)\n",
    "    return probabilities\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(summaries, inputVector):\n",
    "    probabilities=calculateClassProbabilities(summaries, inputVector)\n",
    "    bestLabel, bestProb=None, -1\n",
    "    for classValue, probability in probabilities.items():\n",
    "        if bestLabel is None or probability > bestProb:\n",
    "            bestProb=probability\n",
    "            bestLabel=classValue\n",
    "    return bestLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(summaries, testSet):\n",
    "    predictions=[]\n",
    "    for i in range(len(testSet)):\n",
    "            result=predict(summaries, testSet[i])\n",
    "            predictions.append(result)\n",
    "    return predictions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(testSet, predictions):\n",
    "    correct=0\n",
    "    for i in range(len(testSet)):\n",
    "        if testSet[i][-1]==predictions[i]:\n",
    "            correct+=1\n",
    "    return (correct/float(len(testSet))) * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " the length of the Data Set: 768\n",
      "\n",
      " the Data Set Splitting into Training and Testing\n",
      "\n",
      "\n",
      " Number of Rows in Training Set:514 rows\n",
      "\n",
      " Number of Rows in Testing Set:254 rows\n",
      "\n",
      " First Five Rows of Training Set:\n",
      "\n",
      "[2.0, 130.0, 96.0, 0.0, 0.0, 22.6, 0.268, 21.0, 0.0] \n",
      "\n",
      "[3.0, 100.0, 68.0, 23.0, 81.0, 31.6, 0.949, 28.0, 0.0] \n",
      "\n",
      "[0.0, 129.0, 80.0, 0.0, 0.0, 31.2, 0.703, 29.0, 0.0] \n",
      "\n",
      "[1.0, 128.0, 82.0, 17.0, 183.0, 27.5, 0.115, 22.0, 0.0] \n",
      "\n",
      "[13.0, 152.0, 90.0, 33.0, 29.0, 26.8, 0.731, 43.0, 1.0] \n",
      "\n",
      "\n",
      " First Five Rows of Testing Set:\n",
      "\n",
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0] \n",
      "\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0, 1.0] \n",
      "\n",
      "[5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 0.201, 30.0, 0.0] \n",
      "\n",
      "[2.0, 197.0, 70.0, 45.0, 543.0, 30.5, 0.158, 53.0, 1.0] \n",
      "\n",
      "[10.0, 168.0, 74.0, 0.0, 0.0, 38.0, 0.537, 34.0, 1.0] \n",
      "\n",
      "\n",
      " Model Summaries:\n",
      " {0.0: [(3.31044776119403, 2.97781891449638), (110.54626865671642, 24.97034365768116), (69.25074626865671, 16.21108304565961), (20.065671641791045, 14.848592669445658), (67.57014925373134, 95.09264473298126), (30.43164179104479, 7.897199647122502), (0.42219402985074594, 0.28386234955905093), (30.97313432835821, 11.693536549427632)], 1.0: [(4.955307262569832, 3.754878817372033), (139.97206703910615, 31.495751482282227), (70.68156424581005, 22.284277871091497), (22.083798882681563, 17.95214346952296), (98.58100558659218, 144.49469442106536), (35.30335195530727, 7.121252410375964), (0.5519944134078214, 0.37773643532939244), (37.87709497206704, 10.969645577577753)]}\n",
      "\n",
      " predictions:\n",
      " [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0]\n",
      "\n",
      " Accuracy: 71.25984251968504%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    filename='data5.csv'\n",
    "    splitRatio=0.67\n",
    "    dataset=loadcsv(filename)\n",
    "    \n",
    "    print(\"\\n the length of the Data Set:\",len(dataset))\n",
    "    print(\"\\n the Data Set Splitting into Training and Testing\\n\")\n",
    "    trainingSet, testSet=splitDataset(dataset, splitRatio)\n",
    "    \n",
    "    print('\\n Number of Rows in Training Set:{0} rows'.format(len(trainingSet)))\n",
    "    print('\\n Number of Rows in Testing Set:{0} rows'.format(len(testSet)))\n",
    "    print(\"\\n First Five Rows of Training Set:\\n\")\n",
    "    for i in range(0,5):\n",
    "        print(trainingSet[i],\"\\n\")\n",
    "    print(\"\\n First Five Rows of Testing Set:\\n\")\n",
    "    for i in range(0,5):\n",
    "        print(testSet[i],\"\\n\")\n",
    "    \n",
    "    summaries=summarizeByClass(trainingSet)\n",
    "    print(\"\\n Model Summaries:\\n\",summaries)\n",
    "   \n",
    "    predictions=getPredictions(summaries, testSet)\n",
    "    print(\"\\n predictions:\\n\",predictions)\n",
    "    \n",
    "    accuracy=getAccuracy(testSet, predictions)\n",
    "    print('\\n Accuracy: {0}%'.format(accuracy))\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
